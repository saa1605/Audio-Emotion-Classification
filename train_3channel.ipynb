{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>wav_file</th>\n",
       "      <th>emotion</th>\n",
       "      <th>val</th>\n",
       "      <th>act</th>\n",
       "      <th>dom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6.2901</td>\n",
       "      <td>8.2357</td>\n",
       "      <td>Ses01F_impro01_F000</td>\n",
       "      <td>neu</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10.0100</td>\n",
       "      <td>11.3925</td>\n",
       "      <td>Ses01F_impro01_F001</td>\n",
       "      <td>neu</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>14.8872</td>\n",
       "      <td>18.0175</td>\n",
       "      <td>Ses01F_impro01_F002</td>\n",
       "      <td>neu</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>27.4600</td>\n",
       "      <td>31.4900</td>\n",
       "      <td>Ses01F_impro01_F005</td>\n",
       "      <td>neu</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>85.2700</td>\n",
       "      <td>88.0200</td>\n",
       "      <td>Ses01F_impro01_F012</td>\n",
       "      <td>ang</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  start_time  end_time             wav_file emotion  val  act  dom\n",
       "0      0      6.2901    8.2357  Ses01F_impro01_F000     neu  2.5  2.5  2.5\n",
       "1      1     10.0100   11.3925  Ses01F_impro01_F001     neu  2.5  2.5  2.5\n",
       "2      2     14.8872   18.0175  Ses01F_impro01_F002     neu  2.5  2.5  2.5\n",
       "3      3     27.4600   31.4900  Ses01F_impro01_F005     neu  2.5  3.5  2.0\n",
       "4      4     85.2700   88.0200  Ses01F_impro01_F012     ang  2.0  3.5  3.5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "audio_impro_processed = pd.read_csv('audio_df_improvised.csv')\n",
    "\n",
    "audio_impro_processed.reset_index(inplace=True)\n",
    "audio_impro_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, transforms\n",
    "my_transforms=transforms.Compose([transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                           [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from PIL import Image\n",
    "def get_spectrogram_image(path, my_transforms, med_duration, sr=None, n_fft=2048, hop_length=256, n_mels=128, fmin=100, fmax=15000, top_db=80):\n",
    "    med_duration = int(np.floor(med_duration))\n",
    "    wav, sr = librosa.load(path)\n",
    "    if wav.shape[0]<(med_duration*sr):\n",
    "        wav=np.pad(wav,int(np.ceil(((med_duration*sr)-wav.shape[0])/2)),mode='reflect')\n",
    "    else:\n",
    "        wav=wav[:(med_duration*sr)]\n",
    "    spec=librosa.feature.melspectrogram(wav, sr=sr, n_fft=n_fft,\n",
    "              hop_length=hop_length,n_mels=n_mels)\n",
    "#     librosa.decompose.nn_filter(spec, aggregate=np.median, metric='cosine')\n",
    "    spec_db=librosa.power_to_db(spec)\n",
    "    img=np.stack((spec_db,)*3, axis=-1)\n",
    "    PIL_image = Image.fromarray((img*255).astype(np.uint8))\n",
    "    image=my_transforms(PIL_image)\n",
    "    return PIL_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-f4c15f3cc01e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mIm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = get_spectrogram_image('/data/home/advaitmb/datasets/sentences/' + audio_impro_processed['wav_file'][1] + '.wav', my_transforms, 3)\n",
    "\n",
    "from IPython.display import Image as Im\n",
    "\n",
    "display(Im(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Following code validates on the Male speaker of last Session "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We split the dataframe into train and test segments\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "train, valid = train_test_split(audio_impro_processed, test_size=0.2, shuffle=False)\n",
    "train = audio_impro_processed[:2617]\n",
    "train = shuffle(train)\n",
    "valid = audio_impro_processed[267:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "class IEMOCAP_3Channel(Dataset):\n",
    "    def __init__(self, base, df, in_col, out_col):\n",
    "        self.df = df\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.c2i={}\n",
    "        self.i2c={}\n",
    "        self.categories = sorted(df[out_col].unique())\n",
    "        print(self.categories)\n",
    "        for i, category in enumerate(self.categories):\n",
    "            self.c2i[category]=i\n",
    "            self.i2c[i]=category\n",
    "        for ind in tqdm(range(len(df))):\n",
    "            row = df.iloc[ind]\n",
    "            \n",
    "            #If all files are stored in a folder named sentences\n",
    "            file_path = base + '/' + row[in_col] + '.wav'\n",
    "            \n",
    "            \n",
    "            self.data.append(get_spectrogram_image(file_path, my_transforms, 4))\n",
    "            self.labels.append(self.c2i[row['emotion']])\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2617 [00:00<05:14,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ang', 'hap', 'neu', 'sad']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2617/2617 [06:05<00:00,  7.16it/s]\n",
      "  0%|          | 1/2676 [00:00<05:56,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ang', 'hap', 'neu', 'sad']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 1049/2676 [02:28<03:50,  7.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-528351ed858e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIEMOCAP_3Channel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/home/advaitmb/datasets/sentences'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wav_file'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'emotion'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvalid_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIEMOCAP_3Channel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/data/home/advaitmb/datasets/sentences'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wav_file'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'emotion'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mvalid_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-117-a9d4948eb554>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, base, df, in_col, out_col)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_spectrogram_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_transforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc2i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'emotion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-ed5ee4c88551>\u001b[0m in \u001b[0;36mget_spectrogram_image\u001b[0;34m(path, my_transforms, med_duration, sr, n_fft, hop_length, n_mels, fmin, fmax, top_db)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec_db\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mPIL_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmy_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPIL_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py36/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = IEMOCAP_3Channel('/data/home/advaitmb/datasets/sentences', train, 'wav_file', 'emotion')\n",
    "valid_data = IEMOCAP_3Channel('/data/home/advaitmb/datasets/sentences', valid, 'wav_file', 'emotion')\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=16)\n",
    "valid_loader = DataLoader(valid_data, batch_size=32, shuffle=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    print(data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import librosa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "net = models.resnet18(pretrained=True)\n",
    "net.fc = nn.Linear(512, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda:0')\n",
    "else:\n",
    "    device=torch.device('cpu')\n",
    "\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setlr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return optimizer\n",
    "\n",
    "def lr_decay(optimizer, epoch):\n",
    "    if epoch%10==0:\n",
    "        new_lr = learning_rate / (10**(epoch//10))\n",
    "        optimizer = setlr(optimizer, new_lr)\n",
    "        print(\"Changed learning rate to {}\".format(new_lr))\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 2e-5\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "epochs = 20\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "resnet_train_losses=[]\n",
    "resnet_valid_losses=[]\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train_model(model, loss_fn, train_loader, valid_loader, epochs, optimizer, train_losses, valid_losses, change_lr=None):\n",
    "    \n",
    "    for epoch in tqdm(range(1,epochs+1)):\n",
    "        \n",
    "        model.train()\n",
    "        batch_losses=[]\n",
    "        if change_lr:\n",
    "            optimizer = change_lr(optimizer, epoch)\n",
    "        for i, data in enumerate(train_loader):\n",
    "            x, y = data\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device, dtype=torch.float32)\n",
    "            y = y.to(device, dtype=torch.long)\n",
    "            y_hat = model(x)\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            loss.backward()\n",
    "            batch_losses.append(loss.item())\n",
    "            optimizer.step()\n",
    "        train_losses.append(batch_losses)\n",
    "\n",
    "\n",
    "        print(\"Epoch - {} Train-Loss : {}\".format(epoch, np.mean(train_losses[-1])))\n",
    "        model.eval()\n",
    "        batch_losses=[]\n",
    "        trace_y = []\n",
    "        trace_yhat = []\n",
    "        for i, data in enumerate(valid_loader):\n",
    "            x, y = data\n",
    "            x = x.to(device, dtype=torch.float32)\n",
    "            y = y.to(device, dtype=torch.long)\n",
    "            y_hat = model(x)\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            trace_y.append(y.cpu().detach().numpy())\n",
    "            trace_yhat.append(y_hat.cpu().detach().numpy())      \n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        valid_losses.append(batch_losses)\n",
    "        trace_y = np.concatenate(trace_y)\n",
    "        trace_yhat = np.concatenate(trace_yhat)\n",
    "#         accuracy_unweighted = np.mean(trace_yhat.argmax(axis=1)==trace_y)\n",
    "        accuracy_unweighted = accuracy_score(trace_yhat.argmax(axis=1), trace_y)\n",
    "        accuracy_weighted = balanced_accuracy_score(trace_yhat.argmax(axis=1), trace_y)\n",
    "        print(\"Epoch - {} Valid-Loss : {} Valid-Accuracy Unweighted : {} Valid-Accuracy Weighted {}\".format(epoch, np.mean(valid_losses[-1]), accuracy_unweighted, accuracy_weighted ))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1 Train-Loss : 1.1526133265437148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 1/20 [00:21<06:49, 21.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1 Valid-Loss : 1.1260659803043713 Valid-Accuracy Unweighted : 0.49079754601226994 Valid-Accuracy Weighted 0.3808571364325547\n",
      "Epoch - 2 Train-Loss : 0.8994067747418474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 2/20 [00:43<06:27, 21.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 2 Valid-Loss : 1.0901475277814 Valid-Accuracy Unweighted : 0.5214723926380368 Valid-Accuracy Weighted 0.4270657771535581\n",
      "Epoch - 3 Train-Loss : 0.7532863936773161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|█▌        | 3/20 [01:04<06:07, 21.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 3 Valid-Loss : 1.093750693581321 Valid-Accuracy Unweighted : 0.50920245398773 Valid-Accuracy Weighted 0.39703330379559887\n",
      "Epoch - 4 Train-Loss : 0.6093033301394161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 4/20 [01:26<05:46, 21.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 4 Valid-Loss : 1.1191152334213257 Valid-Accuracy Unweighted : 0.5214723926380368 Valid-Accuracy Weighted 0.42485542724353853\n",
      "Epoch - 5 Train-Loss : 0.46790071667694466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 25%|██▌       | 5/20 [01:48<05:25, 21.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 5 Valid-Loss : 1.247928874059157 Valid-Accuracy Unweighted : 0.44785276073619634 Valid-Accuracy Weighted 0.365725677830941\n",
      "Epoch - 6 Train-Loss : 0.34444876360457116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 6/20 [02:10<05:04, 21.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 6 Valid-Loss : 1.2078542546792463 Valid-Accuracy Unweighted : 0.5245398773006135 Valid-Accuracy Weighted 0.4239266286285205\n",
      "Epoch - 7 Train-Loss : 0.23053764969837376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 35%|███▌      | 7/20 [02:32<04:43, 21.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 7 Valid-Loss : 1.1873490105975757 Valid-Accuracy Unweighted : 0.5184049079754601 Valid-Accuracy Weighted 0.41111636376370064\n",
      "Epoch - 8 Train-Loss : 0.1565257284517695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 8/20 [02:53<04:21, 21.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 8 Valid-Loss : 1.2037314962257037 Valid-Accuracy Unweighted : 0.50920245398773 Valid-Accuracy Weighted 0.3933656008035548\n",
      "Epoch - 9 Train-Loss : 0.11399929906900336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 45%|████▌     | 9/20 [03:15<03:59, 21.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 9 Valid-Loss : 1.3745945475318215 Valid-Accuracy Unweighted : 0.5153374233128835 Valid-Accuracy Weighted 0.4151204955891974\n",
      "Changed learning rate to 2.0000000000000003e-06\n",
      "Epoch - 10 Train-Loss : 0.08669299000828731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 10/20 [03:37<03:38, 21.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 10 Valid-Loss : 1.3452861959284002 Valid-Accuracy Unweighted : 0.49079754601226994 Valid-Accuracy Weighted 0.3970192548137034\n",
      "Epoch - 11 Train-Loss : 0.07367260377036362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 55%|█████▌    | 11/20 [03:59<03:16, 21.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 11 Valid-Loss : 1.416847738352689 Valid-Accuracy Unweighted : 0.49693251533742333 Valid-Accuracy Weighted 0.40094863731656183\n",
      "Epoch - 12 Train-Loss : 0.07065284833675478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 12/20 [04:21<02:55, 21.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 12 Valid-Loss : 1.3680830543691462 Valid-Accuracy Unweighted : 0.49693251533742333 Valid-Accuracy Weighted 0.4005524116923765\n",
      "Epoch - 13 Train-Loss : 0.07044486578826498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 65%|██████▌   | 13/20 [04:43<02:33, 21.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 13 Valid-Loss : 1.3687191876498135 Valid-Accuracy Unweighted : 0.4938650306748466 Valid-Accuracy Weighted 0.39893794638930935\n",
      "Epoch - 14 Train-Loss : 0.0602983506169261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 14/20 [05:05<02:11, 21.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 14 Valid-Loss : 1.3817826075987383 Valid-Accuracy Unweighted : 0.49079754601226994 Valid-Accuracy Weighted 0.3952604515984798\n",
      "Epoch - 15 Train-Loss : 0.06387883538334835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 75%|███████▌  | 15/20 [05:27<01:49, 21.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 15 Valid-Loss : 1.3463781042532488 Valid-Accuracy Unweighted : 0.4723926380368098 Valid-Accuracy Weighted 0.3852395935490901\n",
      "Epoch - 16 Train-Loss : 0.05843615336570798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 16/20 [05:49<01:27, 21.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 16 Valid-Loss : 1.3538819768212058 Valid-Accuracy Unweighted : 0.50920245398773 Valid-Accuracy Weighted 0.40800583391251233\n",
      "Epoch - 17 Train-Loss : 0.055867713898783776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 85%|████████▌ | 17/20 [06:11<01:05, 21.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 17 Valid-Loss : 1.4644842906431719 Valid-Accuracy Unweighted : 0.4754601226993865 Valid-Accuracy Weighted 0.3857612547807054\n",
      "Epoch - 18 Train-Loss : 0.05238335202561646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 18/20 [06:33<00:43, 21.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 18 Valid-Loss : 1.398373159495267 Valid-Accuracy Unweighted : 0.4815950920245399 Valid-Accuracy Weighted 0.3881888440860215\n",
      "Epoch - 19 Train-Loss : 0.053146671276630426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 95%|█████████▌| 19/20 [06:55<00:21, 21.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 19 Valid-Loss : 1.4643620415167375 Valid-Accuracy Unweighted : 0.46319018404907975 Valid-Accuracy Weighted 0.38102583020173975\n",
      "Changed learning rate to 2.0000000000000002e-07\n",
      "Epoch - 20 Train-Loss : 0.050683296898879654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [07:17<00:00, 21.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 20 Valid-Loss : 1.3873401338403875 Valid-Accuracy Unweighted : 0.4785276073619632 Valid-Accuracy Weighted 0.3872326524643962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(net, loss_fn, train_loader, valid_loader, epochs, optimizer, resnet_train_losses, resnet_valid_losses, lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
