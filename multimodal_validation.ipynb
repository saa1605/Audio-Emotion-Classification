{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import librosa\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import librosa\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import noisereduce as nr\n",
    "import IPython\n",
    "from collections import Counter\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "torch.manual_seed(10)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda:0')\n",
    "else:\n",
    "    device=torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setlr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return optimizer\n",
    "\n",
    "def lr_decay(optimizer, epoch, learning_rate):\n",
    "    if epoch%10==0:\n",
    "        new_lr = learning_rate / (10**(epoch//10))\n",
    "        optimizer = setlr(optimizer, new_lr)\n",
    "        print(\"Changed learning rate to {}\".format(new_lr))\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = '/data/home/advaitmb/datasets/sentences/'\n",
    "vocab_size = 2255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('spectrograms.pkl', 'rb') as handle:\n",
    "    (data, labels) = pickle.load(handle)\n",
    "    \n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text_encodings.pkl', 'rb') as handle:\n",
    "    (X, y) = pickle.load(handle)\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Audio(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.y = Y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.from_numpy(self.X[idx][0].astype(np.int32)), self.y[idx], self.X[idx][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>wav_file</th>\n",
       "      <th>emotion</th>\n",
       "      <th>val</th>\n",
       "      <th>act</th>\n",
       "      <th>dom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6.2901</td>\n",
       "      <td>8.2357</td>\n",
       "      <td>Ses01F_impro01_F000</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10.0100</td>\n",
       "      <td>11.3925</td>\n",
       "      <td>Ses01F_impro01_F001</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>14.8872</td>\n",
       "      <td>18.0175</td>\n",
       "      <td>Ses01F_impro01_F002</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>27.4600</td>\n",
       "      <td>31.4900</td>\n",
       "      <td>Ses01F_impro01_F005</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>85.2700</td>\n",
       "      <td>88.0200</td>\n",
       "      <td>Ses01F_impro01_F012</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  start_time  end_time             wav_file emotion  val  act  dom\n",
       "0      0      6.2901    8.2357  Ses01F_impro01_F000       0  2.5  2.5  2.5\n",
       "1      1     10.0100   11.3925  Ses01F_impro01_F001       0  2.5  2.5  2.5\n",
       "2      2     14.8872   18.0175  Ses01F_impro01_F002       0  2.5  2.5  2.5\n",
       "3      3     27.4600   31.4900  Ses01F_impro01_F005       0  2.5  3.5  2.0\n",
       "4      4     85.2700   88.0200  Ses01F_impro01_F012       1  2.0  3.5  3.5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "audio = pd.read_csv('audio_df_improvised.csv')\n",
    "\n",
    "audio.reset_index(inplace=True)\n",
    "audio.emotion = pd.Categorical(pd.factorize(audio.emotion)[0])\n",
    "audio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_text = pd.read_csv('text_df.csv')\n",
    "old_text.emotion = audio.emotion\n",
    "old_text.emotion = pd.Categorical(pd.factorize(old_text.emotion)[0])\n",
    "\n",
    "text = pd.DataFrame()\n",
    "text['data'] = old_text.transcription\n",
    "text['label'] = old_text.emotion\n",
    "\n",
    "#tokenization\n",
    "import spacy\n",
    "tok = spacy.load('en')\n",
    "def tokenize (text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # remove punctuation and numbers\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    return [token.text for token in tok.tokenizer(nopunct)]\n",
    "\n",
    "#count number of occurences of each word\n",
    "counts = Counter()\n",
    "for index, row in text.iterrows():\n",
    "    counts.update(tokenize(row['data']))\n",
    "    \n",
    "    \n",
    "#creating vocabulary\n",
    "vocab2index = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "for word in counts:\n",
    "    vocab2index[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_vectors(glove_file=\".vector_cache/glove.6B.50d.txt\"):\n",
    "    \"\"\"Load the glove word vectors\"\"\"\n",
    "    word_vectors = {}\n",
    "    with open(glove_file) as f:\n",
    "        for line in f:\n",
    "            split = line.split()\n",
    "            word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n",
    "    return word_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_matrix(pretrained, word_counts, emb_size = 50):\n",
    "    \"\"\" Creates embedding matrix from word vectors\"\"\"\n",
    "    vocab_size = len(word_counts) + 2\n",
    "    vocab_to_idx = {}\n",
    "    vocab = [\"\", \"UNK\"]\n",
    "    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n",
    "    W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n",
    "    W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n",
    "    vocab_to_idx[\"UNK\"] = 1\n",
    "    i = 2\n",
    "    for word in word_counts:\n",
    "        if word in word_vecs:\n",
    "            W[i] = word_vecs[word]\n",
    "        else:\n",
    "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "        vocab_to_idx[word] = i\n",
    "        vocab.append(word)\n",
    "        i += 1   \n",
    "    return W, np.array(vocab), vocab_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vecs = load_glove_vectors()\n",
    "pretrained_weights, vocab, vocab2index = get_emb_matrix(word_vecs, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "        self.embeddings.weight.requires_grad = True \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 4)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        lstm_out, (ht, ct) = self.lstm(x)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # just run the init of parent class (nn.Module)\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, 2, padding=1) # input is 1 image, 32 output channels, 5x5 kernel / window\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, 2, padding=1) # input is 32, bc the first layer output 32. Then we say the output will be 64 channels, 5x5 kernel / window\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5, 2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 128, 5, 2, padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear((7*8*128), 512) #flattening.\n",
    "        \n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 4) # 512 in, 2 out bc we're doing 2 classes (dog vs cat).\n",
    "        self.activation = nn.Softmax(dim=1)\n",
    "\n",
    "    def convs(self, x):\n",
    "        # max pooling over 2x2\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) # bc this is our output layer. No activation here.\n",
    "        return self.activation(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, audioModel, textModel):\n",
    "        \n",
    "        # CNN for Audio\n",
    "        super().__init__()\n",
    "\n",
    "        self.audioModel = audioModel.eval()\n",
    "        self.textModel = textModel.eval()\n",
    "        \n",
    "        self.linear = nn.Linear(8, 4)\n",
    "\n",
    "\n",
    "    def forward(self, ax, tx, l):\n",
    "        #Audio \n",
    "        ax = self.audioModel(ax)\n",
    "        tx = self.textModel(tx, l)\n",
    "        \n",
    "        x = torch.cat((ax, tx), dim=1)\n",
    "        x = self.linear(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "def train_model(model, loss_fn, audio_train_loader, text_train_loader, audio_valid_loader, text_valid_loader, epochs, learning_rate, optimizer, train_losses, valid_losses, comment, change_lr=None): \n",
    "    for epoch in range(1,epochs+1):\n",
    "        \n",
    "        model.train()\n",
    "        batch_losses=[]\n",
    "        if change_lr:\n",
    "            optimizer = change_lr(optimizer, epoch, learning_rate)\n",
    "        for a_data, t_data in tqdm(zip(audio_train_loader, text_train_loader)):\n",
    "            ax, ay = a_data\n",
    "            tx, ty, l = t_data\n",
    "            ax = ax.to(device, dtype=torch.float32)\n",
    "            ay = ay.to(device, dtype=torch.long)\n",
    "            tx = tx.to(device, dtype=torch.long)\n",
    "            y_hat = model(ax, tx, l)\n",
    "            loss = loss_fn(y_hat, ay)\n",
    "            loss.backward()\n",
    "            batch_losses.append(loss.item())\n",
    "            optimizer.step()\n",
    "        train_losses.append(batch_losses)\n",
    "\n",
    "        model.eval()\n",
    "        batch_losses=[]\n",
    "        trace_y = []\n",
    "        trace_yhat = []\n",
    "        for a_data, t_data in zip(audio_train_loader, text_train_loader):\n",
    "            ax, ay = a_data\n",
    "            tx, ty, l = t_data\n",
    "            ax = ax.to(device, dtype=torch.float32)\n",
    "            ay = ay.to(device, dtype=torch.long)\n",
    "            tx = tx.to(device, dtype=torch.long)\n",
    "            y_hat = model(ax, tx, l)\n",
    "            loss = loss_fn(y_hat, ay)\n",
    "            trace_y.append(ay.cpu().detach().numpy())\n",
    "            trace_yhat.append(y_hat.cpu().detach().numpy())      \n",
    "            batch_losses.append(loss.item())\n",
    "\n",
    "        valid_losses.append(batch_losses)\n",
    "        trace_y = np.concatenate(trace_y)\n",
    "        trace_yhat = np.concatenate(trace_yhat)\n",
    "        accuracy = np.mean(trace_yhat.argmax(axis=1)==trace_y)\n",
    "        unweighted_accuracy = accuracy_score( trace_yhat.argmax(axis=1), trace_y )\n",
    "        weighted_accuracy = balanced_accuracy_score( trace_yhat.argmax(axis=1), trace_y )\n",
    "        print(\"Epoch - {} Train-Loss : {} Valid-Loss : {}\".format(epoch, 0, np.mean(valid_losses[-1])))\n",
    "        print(\"unweighted_accuracy : {} weighted_accuracy : {}\".format(unweighted_accuracy, weighted_accuracy ))\n",
    "        \n",
    "    return unweighted_accuracy, weighted_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(audio_model, text_model, audio_train_loader, audio_valid_loader, text_train_loader, text_valid_loader, comment):\n",
    "    net = CRNN(audio_model, text_model)\n",
    "    \n",
    "    for param in net.audioModel.parameters():\n",
    "        param.requires_grad=False\n",
    "    for param in net.textModel.parameters():\n",
    "        param.requires_grad=False\n",
    "        \n",
    "    learning_rate = 0.01\n",
    "    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    epochs = 1\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    train_losses=[]\n",
    "    valid_losses=[]\n",
    "    valid_accuracy = []\n",
    "    \n",
    "    uwa, wa = train_model(net, loss_fn, audio_train_loader, text_train_loader, audio_valid_loader, text_valid_loader, epochs, learning_rate, optimizer, train_losses, valid_losses, comment, change_lr=None)\n",
    "    print(\"validation over\")\n",
    "    torch.save(net, comment+'.pth')\n",
    "    del net\n",
    "    \n",
    "    return np.mean(valid_losses[-1]), uwa, wa  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ses_idx = np.array([ range(0,523), range(523, 1052), range(1052, 1680), range(1680, 2214), range(2214, len(audio)) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio = []\n",
    "valid_audio = []\n",
    "train_text = []\n",
    "valid_text = []\n",
    "\n",
    "for i in range(ses_idx.shape[0]):\n",
    "    train_audio.append(Audio(np.delete(data,ses_idx[i], axis=0), np.delete(labels,ses_idx[i], axis=0)))\n",
    "    valid_audio.append(Audio(data[ses_idx[i]], labels[ses_idx[i]]))\n",
    "    \n",
    "    train_text.append(Text(np.delete(X,ses_idx[i], axis=0), np.delete(y,ses_idx[i], axis=0)))\n",
    "    valid_text.append(Text(X[ses_idx[i]], y[ses_idx[i]]))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_model = torch.load('Audio validation : 0.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "uwas = []\n",
    "was = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:03, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1 Train-Loss : 0 Valid-Loss : 0.7958550123791945\n",
      "unweighted_accuracy : 0.9475206611570248 weighted_accuracy : 0.9589548176744487\n",
      "validation over\n"
     ]
    }
   ],
   "source": [
    "audio_train_loader = DataLoader(train_audio[0], batch_size=64, shuffle=False)\n",
    "audio_valid_loader = DataLoader(valid_audio[0], batch_size=64, shuffle=False)\n",
    "text_train_loader = DataLoader(train_text[0], batch_size=64, shuffle=False)\n",
    "text_valid_loader = DataLoader(valid_text[0], batch_size=64, shuffle=False)\n",
    "comment=\"Multimodal validation : {}\".format(0)\n",
    "audio_model = torch.load('Audio validation : 0.pth')\n",
    "text_model = torch.load('Text validation : 0.pth')\n",
    "loss, uwa, wa = validate(audio_model, text_model, audio_train_loader, audio_valid_loader, text_train_loader, text_valid_loader, comment)\n",
    "\n",
    "\n",
    "losses.append(loss)\n",
    "uwas.append(uwa)\n",
    "was.append(wa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:03, 11.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1 Train-Loss : 0 Valid-Loss : 0.7992441873801382\n",
      "unweighted_accuracy : 0.9362054681027341 weighted_accuracy : 0.9433392950932229\n",
      "validation over\n"
     ]
    }
   ],
   "source": [
    "audio_train_loader = DataLoader(train_audio[1], batch_size=64, shuffle=False)\n",
    "audio_valid_loader = DataLoader(valid_audio[1], batch_size=64, shuffle=False)\n",
    "text_train_loader = DataLoader(train_text[1], batch_size=64, shuffle=False)\n",
    "text_valid_loader = DataLoader(valid_text[1], batch_size=64, shuffle=False)\n",
    "comment=\"Multimodal validation : {}\".format(1)\n",
    "audio_model = torch.load('Audio validation : 1.pth')\n",
    "text_model = torch.load('Text validation : 1.pth')\n",
    "loss, uwa, wa = validate(audio_model, text_model, audio_train_loader, audio_valid_loader, text_train_loader, text_valid_loader, comment)\n",
    "\n",
    "\n",
    "losses.append(loss)\n",
    "uwas.append(uwa)\n",
    "was.append(wa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [00:03, 11.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1 Train-Loss : 0 Valid-Loss : 0.938661921668697\n",
      "unweighted_accuracy : 0.8043196544276457 weighted_accuracy : 0.8551610882619295\n",
      "validation over\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/metrics/classification.py:1437: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    }
   ],
   "source": [
    "audio_train_loader = DataLoader(train_audio[2], batch_size=64, shuffle=False)\n",
    "audio_valid_loader = DataLoader(valid_audio[2], batch_size=64, shuffle=False)\n",
    "text_train_loader = DataLoader(train_text[2], batch_size=64, shuffle=False)\n",
    "text_valid_loader = DataLoader(valid_text[2], batch_size=64, shuffle=False)\n",
    "comment=\"Multimodal validation : {}\".format(2)\n",
    "audio_model = torch.load('Audio validation : 2.pth')\n",
    "text_model = torch.load('Text validation : 2.pth')\n",
    "loss, uwa, wa = validate(audio_model, text_model, audio_train_loader, audio_valid_loader, text_train_loader, text_valid_loader, comment)\n",
    "\n",
    "\n",
    "losses.append(loss)\n",
    "uwas.append(uwa)\n",
    "was.append(wa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [00:03, 11.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1 Train-Loss : 0 Valid-Loss : 0.7849988843265333\n",
      "unweighted_accuracy : 0.9601494396014943 weighted_accuracy : 0.9680825636658217\n",
      "validation over\n"
     ]
    }
   ],
   "source": [
    "audio_train_loader = DataLoader(train_audio[3], batch_size=64, shuffle=False)\n",
    "audio_valid_loader = DataLoader(valid_audio[3], batch_size=64, shuffle=False)\n",
    "text_train_loader = DataLoader(train_text[3], batch_size=64, shuffle=False)\n",
    "text_valid_loader = DataLoader(valid_text[3], batch_size=64, shuffle=False)\n",
    "comment=\"Multimodal validation : {}\".format(3)\n",
    "audio_model = torch.load('Audio validation : 3.pth')\n",
    "text_model = torch.load('Text validation : 3.pth')\n",
    "loss, uwa, wa = validate(audio_model, text_model, audio_train_loader, audio_valid_loader, text_train_loader, text_valid_loader, comment)\n",
    "\n",
    "\n",
    "losses.append(loss)\n",
    "uwas.append(uwa)\n",
    "was.append(wa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [00:03, 11.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1 Train-Loss : 0 Valid-Loss : 0.8538094179970878\n",
      "unweighted_accuracy : 0.8857271906052394 weighted_accuracy : 0.9171729262529535\n",
      "validation over\n"
     ]
    }
   ],
   "source": [
    "audio_train_loader = DataLoader(train_audio[4], batch_size=64, shuffle=False)\n",
    "audio_valid_loader = DataLoader(valid_audio[4], batch_size=64, shuffle=False)\n",
    "text_train_loader = DataLoader(train_text[4], batch_size=64, shuffle=False)\n",
    "text_valid_loader = DataLoader(valid_text[4], batch_size=64, shuffle=False)\n",
    "comment=\"Multimodal validation : {}\".format(4)\n",
    "audio_model = torch.load('Audio validation : 4.pth')\n",
    "text_model = torch.load('Text validation : 4.pth')\n",
    "loss, uwa, wa = validate(audio_model, text_model, audio_train_loader, audio_valid_loader, text_train_loader, text_valid_loader, comment)\n",
    "\n",
    "\n",
    "losses.append(loss)\n",
    "uwas.append(uwa)\n",
    "was.append(wa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss : 0.8290179138896039\n",
      "Average Unweighted Accuracy : 0.912265305896957  Average Weighted Accuracy Score : 0.9334835761403891\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean \n",
    "print(\"Average Loss : {}\".format(mean(losses)))\n",
    "print(\"Average Unweighted Accuracy : {}  Average Weighted Accuracy Score : {}\" .format(mean(uwas), mean(was)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
